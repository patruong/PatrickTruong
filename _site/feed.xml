<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-19T14:11:05+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Patrick Truong</title><subtitle>Working towards demystifying biology, finance and other interesting topics</subtitle><entry><title type="html">Independent Component Analysis (ICA) vs Principal Component Analysis (PCA)</title><link href="http://localhost:4000/blog/PCA-vs-ICA" rel="alternate" type="text/html" title="Independent Component Analysis (ICA) vs Principal Component Analysis (PCA)" /><published>2020-10-02T00:00:00+02:00</published><updated>2020-10-02T00:00:00+02:00</updated><id>http://localhost:4000/blog/PCA-vs-ICA</id><content type="html" xml:base="http://localhost:4000/blog/PCA-vs-ICA">&lt;h1 id=&quot;principal-component-analysis-pca&quot;&gt;Principal Component Analysis (PCA)&lt;/h1&gt;
&lt;p&gt;The PCA is a statstical method that uses an orthogonal transformation to convert a set of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. The transformation is done in such a way that the first principal component captures tha largest possible variance, and each succeeding component in turn captures the largest variance possible under the contraint that it is orthogonal (or uncorrelated) with the preceding components. PCA is sensitive to the relative scaling of the original variables, therefor data used with the PCA needs to be normalized.&lt;/p&gt;

&lt;h1 id=&quot;independent-component-analysis-ica&quot;&gt;Independent Component Analysis (ICA)&lt;/h1&gt;
&lt;p&gt;The ICA is a statistical method to find the hidden factors (or latent variables) that yields a set of random variables, measures or signals. The ICA assumes data is multivariate and defines a generative model for the observed data. By the multivariate assumption it is understood that the observed data is a linear mixture of some unknown latent variable, where the latent variables are assumed non-gaussian and mutually independent. The latent variables are the independent components (often called sources or factors found by ICA) of the observed data. The method is often able to identify underlying factors where FA and PCA fail. The ICA is commonly used in signals processing since it aims to capture the underlying independent components of the data.&lt;/p&gt;

&lt;h1 id=&quot;ica-vs-pca&quot;&gt;ICA vs PCA&lt;/h1&gt;
&lt;p&gt;Both PCA and ICA are dimensionality reduction methods to find a set of vectors for the data, that are a linear combination of the basis. In PCA the basis are the vectors that best captures the variance of the data. In ICA the basis are vectors that are independent component of the data. Imagine that the data is a mix of signals and then the ICA basis will have vectors for each independent signal, while the PCA components will capture the variance in descending order, where the components are orthogonal (i.e. uncorrelated).&lt;/p&gt;

&lt;p&gt;In a practical sense, the PCA could be used to find a reduced-rank representation of the data and the ICA could be used to find a representation of the data as independent sub-elements. In other words, the PCA attempts to compress the data and the ICA attempts to seperate the data.&lt;/p&gt;

&lt;iframe src=&quot;https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_blind_source_separation_001.png&quot; width=&quot;600&quot; height=&quot;400&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;Fig 1. Observations of mixed signals and recovered signals with ICA and PCA.&lt;/p&gt;

&lt;p&gt;Code for generating above plot is shown below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-print(__doc__)&quot;&gt;
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal

from sklearn.decomposition import FastICA, PCA


# #############################################################################
# Generate sample data

np.random.seed(0)
n_samples = 2000
time = np.linspace(0, 8, n_samples)

s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal
s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal
s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal

S = np.c_[s1, s2, s3]
S += 0.2 * np.random.normal(size=S.shape)  # Add noise

S /= S.std(axis=0)  # Standardize data
# Mix data
A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix
X = np.dot(S, A.T)  # Generate observations

# Compute ICA
ica = FastICA(n_components=3)
S_ = ica.fit_transform(X)  # Reconstruct signals
A_ = ica.mixing_  # Get estimated mixing matrix

# We can `prove` that the ICA model applies by reverting the unmixing.
assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)

# For comparison, compute PCA
pca = PCA(n_components=3)
H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components

# #############################################################################
# Plot results

plt.figure()

models = [X, S, S_, H]
names = ['Observations (mixed signal)',
         'True Sources',
         'ICA recovered signals',
         'PCA recovered signals']
colors = ['red', 'steelblue', 'orange']

for ii, (model, name) in enumerate(zip(models, names), 1):
    plt.subplot(4, 1, ii)
    plt.title(name)
    for sig, color in zip(model.T, colors):
        plt.plot(sig, color=color)

plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test&lt;/p&gt;</content><author><name></name></author><category term="statistics" /><category term="dimensionality reduction" /><category term="PCA" /><category term="ICA" /><summary type="html">Principal Component Analysis (PCA) The PCA is a statstical method that uses an orthogonal transformation to convert a set of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. The transformation is done in such a way that the first principal component captures tha largest possible variance, and each succeeding component in turn captures the largest variance possible under the contraint that it is orthogonal (or uncorrelated) with the preceding components. PCA is sensitive to the relative scaling of the original variables, therefor data used with the PCA needs to be normalized. Independent Component Analysis (ICA) The ICA is a statistical method to find the hidden factors (or latent variables) that yields a set of random variables, measures or signals. The ICA assumes data is multivariate and defines a generative model for the observed data. By the multivariate assumption it is understood that the observed data is a linear mixture of some unknown latent variable, where the latent variables are assumed non-gaussian and mutually independent. The latent variables are the independent components (often called sources or factors found by ICA) of the observed data. The method is often able to identify underlying factors where FA and PCA fail. The ICA is commonly used in signals processing since it aims to capture the underlying independent components of the data. ICA vs PCA Both PCA and ICA are dimensionality reduction methods to find a set of vectors for the data, that are a linear combination of the basis. In PCA the basis are the vectors that best captures the variance of the data. In ICA the basis are vectors that are independent component of the data. Imagine that the data is a mix of signals and then the ICA basis will have vectors for each independent signal, while the PCA components will capture the variance in descending order, where the components are orthogonal (i.e. uncorrelated). In a practical sense, the PCA could be used to find a reduced-rank representation of the data and the ICA could be used to find a representation of the data as independent sub-elements. In other words, the PCA attempts to compress the data and the ICA attempts to seperate the data. Fig 1. Observations of mixed signals and recovered signals with ICA and PCA. Code for generating above plot is shown below. import numpy as np import matplotlib.pyplot as plt from scipy import signal from sklearn.decomposition import FastICA, PCA # ############################################################################# # Generate sample data np.random.seed(0) n_samples = 2000 time = np.linspace(0, 8, n_samples) s1 = np.sin(2 * time) # Signal 1 : sinusoidal signal s2 = np.sign(np.sin(3 * time)) # Signal 2 : square signal s3 = signal.sawtooth(2 * np.pi * time) # Signal 3: saw tooth signal S = np.c_[s1, s2, s3] S += 0.2 * np.random.normal(size=S.shape) # Add noise S /= S.std(axis=0) # Standardize data # Mix data A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]]) # Mixing matrix X = np.dot(S, A.T) # Generate observations # Compute ICA ica = FastICA(n_components=3) S_ = ica.fit_transform(X) # Reconstruct signals A_ = ica.mixing_ # Get estimated mixing matrix # We can `prove` that the ICA model applies by reverting the unmixing. assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_) # For comparison, compute PCA pca = PCA(n_components=3) H = pca.fit_transform(X) # Reconstruct signals based on orthogonal components # ############################################################################# # Plot results plt.figure() models = [X, S, S_, H] names = ['Observations (mixed signal)', 'True Sources', 'ICA recovered signals', 'PCA recovered signals'] colors = ['red', 'steelblue', 'orange'] for ii, (model, name) in enumerate(zip(models, names), 1): plt.subplot(4, 1, ii) plt.title(name) for sig, color in zip(model.T, colors): plt.plot(sig, color=color) plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46) plt.show() test</summary></entry></feed>